A NICER View of PSR J0030+0451:
    Nested Samples for Millisecond Pulsar Parameter Estimation

Authors:
    Riley T.E., Watts A.L., Bogdanov S., Ray P.S., Ludlam R.M.,
    Guillot S., Arzoumanian Z., Baker C.L., Bilous A.V., Chakrabarty D.,
    Gendreau K.C., Harding A.K., Ho W.C.G., Lattimer J.M., Morsink S.M.,
    Strohmayer T.E.

Accompanying article:
    Riley et al. 2019, ApJL, 887, L21 (DOI: 10.3847/2041-8213/ab481c)

Deposit contents
================

Sample files
------------

There is one directory per model (with corresponding table in the Letter):
    * ST-S
        > run1
        > run2
    * ST-U
        > run1
        > run2
        > run3
    * CDT-U
        > run1
        > run_with_attempted_reparametrisation (manual termination;
            see Table 7 in the Letter)
    * ST+CST
        > run1
        > run2
    * ST+EST
        > run1
    * ST+PST
        > run1

In each model directory there are one or more run directories.
In each run directory there are a set of files generated by MultiNest v3.11.
The files contain information about one or more nested sampling processes.

The file names contain conventional identifiers appended by MultiNest.
Other information can be found in the filename too.
As an example, consider the ST-S model path:
    ST_S/run1/run1_nlive1000_eff0.3_noCONST_noMM_noIS_tol-1.txt

The separated elements here are:
    * the run ID "run1"
    * the number of live points "nlive"=1000
    * the efficiency setting "eff"=0.3
    * the state of constant efficiency variant "CONST"=False
    * the state of mode-separation variant "MM"=False
    * the state of the importance nested sampling augmentation "IS"=False
    * the termination tolerance condition "tol"=10^{-1}

Other files generated by MultiNest on path "ST_S/run1/" are have the same
prefix "run1_nlive1000_eff0.3_noCONST_noMM_noIS_tol-1", but have been
automatically appended with identifiers and file extensions:
    * .txt = weighted nested sample file
    * ev.dat = accumulated dead points (parameter vectors) ordered in likelihood
    * resume.dat = information for run resume, written to disk at higher cadence
    * stats.dat = estimators (mode breakdown if "MM" variant activated)
    * summary.txt = largely similar to stats.dat but not as human readable
    * post_equal_weights.dat = equally weighted posterior samples (more MC noise)
    * live.points = current live points in the unit hypercube (sampling space)
    * phys_live.points = current live points in the parameter space
    * phys_live-birth.txt = birth iteration numbers of current live points
    * dead-birth.txt = birth iteration numbers of dead points

The "birth" files are generated by MultiNest >= v3.11, and are needed for
thread decomposition and thus Monte Carlo error analysis via bootstrapping, and
for run combination (see below).

In some cases the sampling was run with "IS"=True, however this did not lead
to increased evidence estimation accuracy. This is because Importance nested
sampling is not compatible for evidence estimation with the way we typically
implement the joint prior density. Thus we do not consider it to be activated
because it is not used for any inferences in the journal article. Moreover,
the augmented sampling process writes to disk all the rejected points too,
leading to additional information albeit requiring much larger disk space
(up to 5 GB). These files are archived but some of the other files refer to
additional estimators based on importance nested sampling.

Many of these files contain vectors as a subset of a row, with other elements
of a row being associated with a given vector. The vectors are in some cases
parameter vectors in a physical space, whilst in other cases they are points
in a unit hypercube. The vectors are ordered, and a vector in the unit
hypercube maps to a vector in the physical space. The ordering is defined
by the likelihood and prior callbacks. To retrieve the ordering for
each model, you may consult the custom prior subclass for each model (see
the "Model & job scripts" section below), wherein the parameter vector is
given in the class docstring.

Information about these sampling process settings can be located in the
accompanying Letter and in other literature, first and foremost:
    * README @ https://github.com/farhanferoz/MultiNest
    * Feroz+ (2009) for MultiNest
    * Feroz+ (2013) for importance nested sampling with MultiNest
    * Higson+ (2018a,b) for error analysis and run combination by
        thread decomposition, for diagnostic tests, and for open-source
        software "nestcheck" to facilitate this post-processing
Other recommended reading includes:
    * Skilling (2006) for seminal work on nested sampling
    * Handley+ (2015) for digestible nested sampling and prior implementation
        theory, and for the PolyChord nested sampling algorithm variant
    * Speagle (2019) for a near comprehensive review of nested sampling
        together with a pure Python implementation "dynesty" with extensive API
        documentation, tutorials, and theory
    * Handley (2019) for open-source software "anesthetic" was not used in
        this work but would have been a useful tool
See the end of this file for references.

Typically in each model model directory there is an "output" directory that
contains verbose output generated by MultiNest during each run. This
information is closely monitored, along with a subset of the above files,
during a run to gain understanding of how the process is behaving.
(This is vague, but we must refer to the literature to learn more about the
nested sampling processes and the MultiNest implementation.)


Model & job scripts
-------------------

The likelihood and prior were implemented as callbacks using v0.1 of X-PSI.
X-PSI is hosted @ https://github.com/ThomasEdwardRiley/xpsi
Documentation is hosted at https://thomasedwardriley.github.io/xpsi/
If you need the specific version v0.1, you can checkout the tag on the master
branch of the repo.

NB: Although v0.1 was used, it is more accurate to say that parts of the
code were developed during the modelling process, before a more rigorous
versioning system was invoked and the project moved to an open-source repo on
GitHub. This means that in output files, it is usually
stated that v1.0.0 of X-PSI was imported, which, albeit loosely, corresponds
to tag v0.1 in the repo, which is the checkpoint of X-PSI upon submission
of the journal article. Unfortunately, it is now impossible to match every part
of the modelling effort to an exact image of the repository.

In each model directory there a set of Python modules together with an empty
"__init__.py".
These files contain the custom subclasses that constitute the model.
All but one of the files are associated with the likelihood function.
The remaining file defines the joint prior distribution together with
the transformation from the unit hypercube to a physical space for likelihood
evaluation.

In each model directory there is "mains" directory that contains the main
Python scripts that are passed to an MPI executable such as mpiexec.
These scripts import the custom subclasses and perform initialisation
of the likelihood callable and prior callable.
Both callables are then passed as callbacks to MultiNest via PyMultiNest
(https://github.com/JohannesBuchner/PyMultiNest).

In each model there is a "jobs" directory that contains the simple job bash
scripts used with the Slurm batch scheduler on the SURFsara Cartesius
supercomputer.
Such a script can in principle be translated straightforwardly to other systems.

For runs that were resumed, there is typically a separate main script and job
script for the initial stage of the process and each resume. In some cases
there is a "short" script, meaning that the run was started as a short test
job, and then resumed with more resources.


Summary
=======

The information archived here should in principle permit reproduction of
all or parts of the work.

In practice we appreciate that the model scripts are not pristine and verbose,
and v0.1 of X-PSI was not specifically designed to make the API as easy to use
as possible. Improvements have been made since then, and will continue to be
made actively on GitHub.

Finally, if you think there are missing files that you need, you can contact
us using the details below. Unfortunately, some "output" files or job scripts
may have been lost along the way, but should not be critically important,
being largely the same as other surviving scripts and files.


Contact
=======

If you have questions, comments, or requests, please contact:
Thomas E. Riley, t.riley.phd@gmail.com
(in cc) Anna L. Watts, A.L.Watts@uva.nl
(in cc) Slavko Bogdanov, slavko@astro.columbia.edu

These contacts are the first three authors of the accompanying journal article.
This should better ensure awareness of problems and in principle a faster
response. You are of course free to contact other authors if you are unsure
about where to send your message, but you may be redirected.

Once we have established the nature of the message we can determine who is
best positioned to respond and continue correspondence as appropriate.


References
==========

Skilling J. 2006, doi:10.1214/06-BA127
Feroz, F., Hobson, M. P., & Bridges, M. 2009, MNRAS, 398, 1601
Feroz, F., Hobson, M. P., Cameron, E., & Pettitt, A. N. 2013,
    arXiv e-prints, arXiv:1306.2144
Handley, W. J., Hobson, M. P., & Lasenby, A. N. 2015, MNRAS, 453, 4384
Higson, E., Handley, W., Hobson, M., & Lasenby, A. 2018a,
    Statistics and Computing, doi:10.1007/s11222-018-9844-0
â€”. 2018b, Bayesian Analysis, 13, 873
Speagle, J. S. 2019, arXiv e-prints, arXiv:1904.02180
Handley, W. J. 2019, doi:10.21105/joss.01414

